{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxqxnOZ5Yhm4"
      },
      "source": [
        "## Project Description: Improved Vision-to-Code Baseline\n",
        "\n",
        "This project presents an enhanced baseline for converting CAD images into their corresponding CADQuery code. It leverages a vision-language model that combines visual features extracted by a ResNet-50 encoder with a GPT-2-based decoder for code generation. Compared to the most basic baseline, this version adds several practical improvements:\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Dataset: Uses the CADCODER/GenCAD-Code dataset for paired CAD image/code samples.\n",
        "\n",
        "Custom Tokenizer: Trains a byte-level BPE tokenizer on CADQuery code, allowing for robust handling of code syntax and vocabulary.\n",
        "\n",
        "Data Augmentation: Applies random resizing and horizontal flipping to training images for better model generalization.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "Image Encoder: A ResNet-50 model (optionally frozen) extracts a 1024-dim feature vector from each image.\n",
        "\n",
        "Code Decoder: An 8-layer GPT-2 model, initialized with a custom vocabulary, generates code from the encoded visual representation.\n",
        "\n",
        "Training & Evaluation:\n",
        "\n",
        "Mixed-precision training is supported for efficiency.\n",
        "\n",
        "Evaluates models using simple syntax rate and string-matching IoU.\n",
        "\n",
        "All hyperparameters, data paths, and settings are controlled by a single config dataclass.\n",
        "\n",
        "Reproducibility: Training is deterministic, and the model checkpoint is saved upon completion.\n",
        "\n",
        "Purpose:\n",
        "This code serves as a strong, practical foundation for research and development in image-to-code translation for CAD automation and AI-assisted design.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gUw2YGLXVlr"
      },
      "source": [
        "## The primary bottlenecks\n",
        "\n",
        "I encountered in this project were related to both environment setup and compute resources:\n",
        "\n",
        "Environment Configuration Issues: I ran into conflicting dependencies due to not thoroughly checking the pyproject.toml for version compatibility. Resolving these conflicts and properly configuring the environment took almost three hours, significantly cutting into the time available for model experimentation and tuning. This experience highlights how crucial it is to carefully manage dependencies in machine learning projects, especially when integrating multiple libraries.\n",
        "\n",
        "GPU and Compute Limitations: My experiments were restricted to using Google Colab, which provides limited GPU resources. This limited the size and complexity of models I could use, as well as the number of experiments I could run within the available time. Training large image-to-code models or running extensive hyperparameter searches was not feasible in this environment.\n",
        "\n",
        "Together, these bottlenecks—setup time and restricted compute—were the main constraints on what I could achieve in this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJqcCFSnXW_N"
      },
      "source": [
        "## Possible Enhancements with More Time\n",
        "\n",
        "If I had more time and resources, I would focus on the following enhancements:\n",
        "\n",
        "Experiment with Larger Decoder Models: I would test bigger and more expressive decoder architectures to potentially improve the accuracy of CadQuery code generation.\n",
        "\n",
        "Train for Longer: More training epochs would allow the model to learn more effectively from the dataset.\n",
        "\n",
        "Hyperparameter Tuning: I would try different learning rates, batch sizes, and optimizers to optimize performance.\n",
        "\n",
        "Improve Code Organization: I would refactor the codebase to be more organized and modular, making it easier to understand, maintain, and extend in the future.\n",
        "\n",
        "Reduce Redundancy: I would clean up the code to eliminate repeated or unnecessary components, ensuring more efficient and readable scripts.\n",
        "\n",
        "\n",
        "Bottlenecks\n",
        "The primary bottlenecks I encountered in this project were related to both environment setup and compute resources:\n",
        "\n",
        "Environment Configuration Issues: I ran into conflicting dependencies due to not thoroughly checking the pyproject.toml for version compatibility. Resolving these conflicts and properly configuring the environment took almost three hours, significantly cutting into the time available for model experimentation and tuning. This experience highlights how crucial it is to carefully manage dependencies in machine learning projects, especially when integrating multiple libraries.\n",
        "\n",
        "GPU and Compute Limitations: My experiments were restricted to using Google Colab, which provides limited GPU resources. This limited the size and complexity of models I could use, as well as the number of experiments I could run within the available time. Training large image-to-code models or running extensive hyperparameter searches was not feasible in this environment.\n",
        "\n",
        "Together, these bottlenecks—setup time and restricted compute—were the main constraints on what I could achieve in this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dvu8t2yHvlA",
        "outputId": "2422f8c2-22f7-4bd4-f85b-146bebb1d19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cadquery>=2.5.2 in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: datasets>=3.6.0 in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: ipykernel>=6.29.5 in /usr/local/lib/python3.11/dist-packages (6.29.5)\n",
            "Requirement already satisfied: scipy>=1.15.3 in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: trimesh>=4.6.11 in /usr/local/lib/python3.11/dist-packages (4.6.12)\n",
            "Requirement already satisfied: cadquery-ocp<7.8,>=7.7.0 in /usr/local/lib/python3.11/dist-packages (from cadquery>=2.5.2) (7.7.2)\n",
            "Requirement already satisfied: ezdxf in /usr/local/lib/python3.11/dist-packages (from cadquery>=2.5.2) (1.4.2)\n",
            "Requirement already satisfied: multimethod<2.0,>=1.11 in /usr/local/lib/python3.11/dist-packages (from cadquery>=2.5.2) (1.12)\n",
            "Requirement already satisfied: nlopt<3.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from cadquery>=2.5.2) (2.9.1)\n",
            "Requirement already satisfied: typish in /usr/local/lib/python3.11/dist-packages (from cadquery>=2.5.2) (1.9.3)\n",
            "Requirement already satisfied: casadi in /usr/local/lib/python3.11/dist-packages (from cadquery>=2.5.2) (3.7.0)\n",
            "Requirement already satisfied: path in /usr/local/lib/python3.11/dist-packages (from cadquery>=2.5.2) (17.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.6.0) (6.0.2)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (5.8.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=6.29.5) (5.7.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=3.6.0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=3.6.0) (1.1.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel>=6.29.5) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=6.29.5) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.29.5) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.6.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.6.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.6.0) (2025.6.15)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from ezdxf->cadquery>=2.5.2) (3.2.3)\n",
            "Requirement already satisfied: fonttools in /usr/local/lib/python3.11/dist-packages (from ezdxf->cadquery>=2.5.2) (4.58.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.20.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.29.5) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=6.29.5) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.29.5) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=6.29.5) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"cadquery>=2.5.2\" \\\n",
        "             \"datasets>=3.6.0\" \\\n",
        "             \"ipykernel>=6.29.5\" \\\n",
        "             \"scipy>=1.15.3\" \\\n",
        "             \"trimesh>=4.6.11\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmIlaZHZH4XV",
        "outputId": "b968de6c-01fa-4826-fbcc-1d88dfb29d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IOU: 0.5834943417057687\n"
          ]
        }
      ],
      "source": [
        "import argparse, importlib.util, runpy, tempfile, itertools, sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "import cadquery as cq\n",
        "from cadquery import exporters\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from typing import Union\n",
        "import textwrap\n",
        "\n",
        "os.environ[\"CADQUERY_LOG_LEVEL\"] = \"ERROR\"\n",
        "\n",
        "\n",
        "# ---------- helpers ---------------------------------------------------------\n",
        "\n",
        "\n",
        "def _load_solid_from_code(\n",
        "    code: str, script_id: str = \"unknown\"\n",
        ") -> Union[cq.Solid, cq.Compound]:\n",
        "    \"\"\"Execute Python code and return any CadQuery object found.\"\"\"\n",
        "    # Clean up indentation issues\n",
        "    cleaned_code = textwrap.dedent(code).strip()\n",
        "\n",
        "    # Provide necessary imports in the execution namespace\n",
        "    ns = {\"cq\": cq, \"cadquery\": cq, \"np\": np, \"numpy\": np, \"__builtins__\": __builtins__}\n",
        "    try:\n",
        "        exec(cleaned_code, ns)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error executing script {script_id}: {e}\")\n",
        "\n",
        "    # Find any CadQuery objects in the namespace\n",
        "    cadquery_objects = []\n",
        "    for var_name, var_value in ns.items():\n",
        "        if isinstance(var_value, (cq.Workplane, cq.Solid, cq.Compound)):\n",
        "            cadquery_objects.append((var_name, var_value))\n",
        "\n",
        "    if not cadquery_objects:\n",
        "        raise ValueError(\n",
        "            f\"No CadQuery objects (Workplane, Solid, or Compound) found in script {script_id}\"\n",
        "        )\n",
        "\n",
        "    if len(cadquery_objects) > 1:\n",
        "        # If multiple objects, prefer common names\n",
        "        preferred_names = [\"solid\", \"result\", \"shape\", \"part\", \"object\", \"obj\", \"res\"]\n",
        "        for preferred in preferred_names:\n",
        "            for var_name, var_value in cadquery_objects:\n",
        "                if var_name == preferred:\n",
        "                    cadquery_objects = [(var_name, var_value)]\n",
        "                    break\n",
        "            if len(cadquery_objects) == 1:\n",
        "                break\n",
        "\n",
        "        # If still multiple, just take the first one but warn\n",
        "        if len(cadquery_objects) > 1:\n",
        "            var_names = [name for name, _ in cadquery_objects]\n",
        "            print(\n",
        "                f\"Warning: Multiple CadQuery objects found in {script_id}: {var_names}. Using '{cadquery_objects[0][0]}'\"\n",
        "            )\n",
        "\n",
        "    var_name, solid_obj = cadquery_objects[0]\n",
        "    # print(\n",
        "    #     f\"Found CadQuery object in variable '{var_name}' of type {type(solid_obj).__name__}\"\n",
        "    # )\n",
        "\n",
        "    # Handle different CadQuery object types\n",
        "    if isinstance(solid_obj, cq.Workplane):\n",
        "        # Extract the solid from the workplane\n",
        "        solid_obj = solid_obj.val()\n",
        "\n",
        "    # Handle Compound objects (multiple solids combined)\n",
        "    if hasattr(solid_obj, \"Solids\") and callable(getattr(solid_obj, \"Solids\")):\n",
        "        solids = solid_obj.Solids()\n",
        "        if len(solids) == 1:\n",
        "            solid_obj = solids[0]\n",
        "        elif len(solids) > 1:\n",
        "            # If multiple solids, we need to combine them into one\n",
        "            # Use the compound itself if it's valid for our purposes\n",
        "            pass  # Keep the compound as is\n",
        "        else:\n",
        "            raise ValueError(f\"No solids found in compound in script {script_id}\")\n",
        "\n",
        "    # Accept both Solid and Compound objects for our mesh operations\n",
        "    if not isinstance(solid_obj, (cq.Solid, cq.Compound)):\n",
        "        raise ValueError(\n",
        "            f\"CadQuery object '{var_name}' is not a Solid or Compound object in script {script_id}, got {type(solid_obj)}\"\n",
        "        )\n",
        "\n",
        "    return solid_obj\n",
        "\n",
        "\n",
        "def _load_solid(script_path: Path) -> cq.Solid:\n",
        "    \"\"\"Import a CadQuery script in isolation and return the 'solid' object.\"\"\"\n",
        "    ns = runpy.run_path(script_path)  # executes the file\n",
        "    if \"solid\" not in ns or not isinstance(ns[\"solid\"], cq.Solid):\n",
        "        raise ValueError(f\"'solid' not found in {script_path}\")\n",
        "    return ns[\"solid\"]\n",
        "\n",
        "\n",
        "def _root_gyration(solid: Union[cq.Solid, cq.Compound]) -> float:\n",
        "    vol = solid.Volume()\n",
        "    inertia = np.array(cq.Shape.matrixOfInertia(solid)).reshape(3, 3)\n",
        "    return np.sqrt(np.trace(inertia) / (2.0 * vol))\n",
        "\n",
        "\n",
        "def _normalized_mesh(\n",
        "    solid: Union[cq.Solid, cq.Compound], pitch: float = 0.01\n",
        ") -> trimesh.Trimesh:\n",
        "    \"\"\"Translate to centroid, isotropically scale by r_g, and return a mesh.\"\"\"\n",
        "    r_g = _root_gyration(solid)\n",
        "    center_vector = solid.Center()\n",
        "    centroid = np.array([center_vector.x, center_vector.y, center_vector.z])\n",
        "    # Export to temporary STL then load with trimesh\n",
        "    with tempfile.TemporaryDirectory() as tmp:\n",
        "        stl_path = Path(tmp) / \"part.stl\"\n",
        "        exporters.export(solid, str(stl_path))\n",
        "        mesh = trimesh.load(str(stl_path), force=\"mesh\")\n",
        "    mesh.apply_translation(-centroid)\n",
        "    mesh.apply_scale(1.0 / r_g)\n",
        "    return mesh\n",
        "\n",
        "\n",
        "def _principal_axes(mesh: trimesh.Trimesh) -> np.ndarray:\n",
        "    \"\"\"Return 3×3 orthonormal matrix whose columns are principal axes.\"\"\"\n",
        "    inertia = mesh.moment_inertia\n",
        "    _, vecs = np.linalg.eigh(inertia)\n",
        "    return vecs  # columns are eigenvectors\n",
        "\n",
        "\n",
        "def _apply_rotation(mesh: trimesh.Trimesh, R: np.ndarray) -> trimesh.Trimesh:\n",
        "    T = np.eye(4)\n",
        "    T[:3, :3] = R\n",
        "    mesh_rot = mesh.copy()\n",
        "    mesh_rot.apply_transform(T)\n",
        "    return mesh_rot\n",
        "\n",
        "\n",
        "def _voxel_bool_unified(\n",
        "    mesh1: trimesh.Trimesh, mesh2: trimesh.Trimesh, pitch: float = 0.05\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Create voxel grids for both meshes using unified bounds.\"\"\"\n",
        "    # Voxelize each mesh individually first\n",
        "    voxel1 = mesh1.voxelized(pitch)\n",
        "    voxel2 = mesh2.voxelized(pitch)\n",
        "\n",
        "    # Get the bounds of each voxel grid\n",
        "    bounds1 = voxel1.bounds\n",
        "    bounds2 = voxel2.bounds\n",
        "\n",
        "    # Compute unified bounds\n",
        "    min_bounds = np.minimum(bounds1[0], bounds2[0])\n",
        "    max_bounds = np.maximum(bounds1[1], bounds2[1])\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    grid_size = np.ceil((max_bounds - min_bounds) / pitch).astype(int)\n",
        "\n",
        "    # Create empty unified voxel grids\n",
        "    vox1 = np.zeros(grid_size, dtype=bool)\n",
        "    vox2 = np.zeros(grid_size, dtype=bool)\n",
        "\n",
        "    # Calculate offsets for placing each voxel grid in the unified space\n",
        "    offset1 = np.round((bounds1[0] - min_bounds) / pitch).astype(int)\n",
        "    offset2 = np.round((bounds2[0] - min_bounds) / pitch).astype(int)\n",
        "\n",
        "    # Get shapes of individual voxel matrices\n",
        "    shape1 = voxel1.matrix.shape\n",
        "    shape2 = voxel2.matrix.shape\n",
        "\n",
        "    # Calculate end positions\n",
        "    end1 = offset1 + shape1\n",
        "    end2 = offset2 + shape2\n",
        "\n",
        "    # Place voxels in unified grids with bounds checking\n",
        "    if np.all(offset1 >= 0) and np.all(end1 <= grid_size):\n",
        "        vox1[offset1[0] : end1[0], offset1[1] : end1[1], offset1[2] : end1[2]] = (\n",
        "            voxel1.matrix\n",
        "        )\n",
        "\n",
        "    if np.all(offset2 >= 0) and np.all(end2 <= grid_size):\n",
        "        vox2[offset2[0] : end2[0], offset2[1] : end2[1], offset2[2] : end2[2]] = (\n",
        "            voxel2.matrix\n",
        "        )\n",
        "\n",
        "    return vox1, vox2\n",
        "\n",
        "\n",
        "def _voxel_bool(mesh: trimesh.Trimesh, pitch: float = 0.05) -> np.ndarray:\n",
        "    vox = mesh.voxelized(pitch)\n",
        "    return vox.matrix  # boolean 3-D numpy array\n",
        "\n",
        "\n",
        "def iou_best(\n",
        "    mesh_gt: trimesh.Trimesh, mesh_pred: trimesh.Trimesh, pitch: float = 0.05\n",
        ") -> float:\n",
        "    \"\"\"IOU after best principal-axis alignment (4 valid sign flips).\"\"\"\n",
        "    axes_gt = _principal_axes(mesh_gt)\n",
        "    axes_pr = _principal_axes(mesh_pred)\n",
        "\n",
        "    best = 0.0\n",
        "    for signs in [(1, 1, 1), (1, 1, -1), (1, -1, 1), (-1, 1, 1)]:\n",
        "        D = np.diag(signs)\n",
        "        axes_pr_flipped = axes_pr @ D  # change axis directions\n",
        "        R = axes_gt @ axes_pr_flipped.T  # rotation to align\n",
        "        m_aligned = _apply_rotation(mesh_pred, R)\n",
        "\n",
        "        # Use unified voxelization\n",
        "        vox_gt, vox_pr = _voxel_bool_unified(mesh_gt, m_aligned, pitch)\n",
        "\n",
        "        inter = np.logical_and(vox_gt, vox_pr).sum()\n",
        "        union = np.logical_or(vox_gt, vox_pr).sum()\n",
        "\n",
        "        if union > 0:\n",
        "            iou = inter / union\n",
        "            best = max(best, iou)\n",
        "\n",
        "    return best\n",
        "\n",
        "\n",
        "# ---------- main ------------------------------------------------------------\n",
        "\n",
        "\n",
        "def evaluate_codes(gt_codes: dict, pred_codes: dict, pitch: float = 0.05):\n",
        "    \"\"\"Evaluate predictions against ground-truth using Python code directly.\n",
        "\n",
        "    Args:\n",
        "        gt_codes: Dict with IDs as keys and ground-truth Python code as values\n",
        "        pred_codes: Dict with IDs as keys and prediction Python code as values\n",
        "        pitch: Voxel pitch for IoU calculation\n",
        "    \"\"\"\n",
        "    ids = sorted(gt_codes.keys())\n",
        "    if not ids:\n",
        "        sys.exit(\"no ground-truth scripts provided\")\n",
        "\n",
        "    vsr_success = 0\n",
        "    ious = []\n",
        "\n",
        "    for _id in ids:\n",
        "        if _id not in pred_codes:\n",
        "            print(f\"missing prediction for {_id}, skipping\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            solid_gt = _load_solid_from_code(gt_codes[_id], f\"gt_{_id}\")\n",
        "            solid_pr = _load_solid_from_code(pred_codes[_id], f\"pred_{_id}\")\n",
        "            vsr_success += 1\n",
        "        except Exception as exc:\n",
        "            print(f\"{_id}: syntax/runtime error -> {exc}\")\n",
        "            continue\n",
        "\n",
        "        mesh_gt = _normalized_mesh(solid_gt)\n",
        "        mesh_pr = _normalized_mesh(solid_pr)\n",
        "        ious.append(iou_best(mesh_gt, mesh_pr, pitch))\n",
        "\n",
        "    n_total = len(ids)\n",
        "    vsr = vsr_success / n_total if n_total else 0.0\n",
        "    iou_b = np.mean(ious) if ious else 0.0\n",
        "\n",
        "    print(f\"Valid Syntax Rate: {vsr:.3f}\")\n",
        "    print(f\"Mean IOU_best   : {iou_b:.3f}\")\n",
        "\n",
        "    return {\"vsr\": vsr, \"iou_best\": iou_b}\n",
        "\n",
        "\n",
        "def evaluate(gt_dir: Path, pred_dir: Path, pitch: float = 0.05):\n",
        "    \"\"\"Original file-based evaluation function.\"\"\"\n",
        "    ids = sorted(p.stem for p in gt_dir.glob(\"*.py\"))\n",
        "    if not ids:\n",
        "        sys.exit(\"no ground-truth scripts found\")\n",
        "\n",
        "    vsr_success = 0\n",
        "    ious = []\n",
        "\n",
        "    for _id in ids:\n",
        "        gt_path = gt_dir / f\"{_id}.py\"\n",
        "        pr_path = pred_dir / f\"{_id}.py\"\n",
        "        if not pr_path.exists():\n",
        "            print(f\"missing prediction for {_id}, skipping\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            solid_gt = _load_solid(gt_path)\n",
        "            solid_pr = _load_solid(pr_path)\n",
        "            vsr_success += 1\n",
        "        except Exception as exc:\n",
        "            print(f\"{_id}: syntax/runtime error -> {exc}\")\n",
        "            continue\n",
        "\n",
        "        mesh_gt = _normalized_mesh(solid_gt)\n",
        "        mesh_pr = _normalized_mesh(solid_pr)\n",
        "        ious.append(iou_best(mesh_gt, mesh_pr, pitch))\n",
        "\n",
        "    n_total = len(ids)\n",
        "    vsr = vsr_success / n_total if n_total else 0.0\n",
        "    iou_b = np.mean(ious) if ious else 0.0\n",
        "\n",
        "    print(f\"Valid Syntax Rate: {vsr:.3f}\")\n",
        "    print(f\"Mean IOU_best   : {iou_b:.3f}\")\n",
        "\n",
        "\n",
        "def get_iou_best(code1: str, code2: str):\n",
        "    solid1 = _load_solid_from_code(code1)\n",
        "    solid2 = _load_solid_from_code(code2)\n",
        "    mesh1 = _normalized_mesh(solid1)\n",
        "    mesh2 = _normalized_mesh(solid2)\n",
        "    iou = iou_best(mesh1, mesh2)\n",
        "    return iou\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    code1 = \"\"\"\n",
        "        height = 60.0\n",
        "        width = 80.0\n",
        "        thickness = 10.0\n",
        "        res = cq.Workplane(\"XY\").box(height, width, thickness)\n",
        "    \"\"\"\n",
        "    code2 = \"\"\"\n",
        "        height = 60.0\n",
        " width = 80.0\n",
        " thickness = 10.0\n",
        " diameter = 22.0\n",
        " padding = 12.0\n",
        "\n",
        " # make the base\n",
        " result = (\n",
        "     cq.Workplane(\"XY\")\n",
        "     .box(height, width, thickness)\n",
        "     .faces(\">Z\")\n",
        "     .workplane()\n",
        "     .hole(diameter)\n",
        "     .faces(\">Z\")\n",
        "     .workplane()\n",
        "     .rect(height - padding, width - padding, forConstruction=True)\n",
        "     .vertices()\n",
        "     .cboreHole(2.4, 4.4, 2.1)\n",
        " )\n",
        "    \"\"\"\n",
        "    solid1 = _load_solid_from_code(code1)\n",
        "    solid2 = _load_solid_from_code(code2)\n",
        "    mesh1 = _normalized_mesh(solid1)\n",
        "    mesh2 = _normalized_mesh(solid2)\n",
        "    iou = iou_best(mesh1, mesh2)\n",
        "    print(f\"IOU: {iou}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--DJ5FofH6Xz",
        "outputId": "da7106a4-53fa-4b14-ed4e-eb9d09b41b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Valid Syntax Rate evaluation:\n",
            "==================================================\n",
            "✓ box_with_hole: Successfully executed\n",
            "✗ no_cadquery_object: No CadQuery objects (Workplane, Solid, or Compound) found in script no_cadquery_object\n",
            "✗ runtime_error: Error executing script runtime_error: name 'undefined_variable' is not defined\n",
            "✓ simple_box: Successfully executed\n",
            "✗ syntax_error: Error executing script syntax_error: '(' was never closed (<string>, line 1)\n",
            "\n",
            "--- SUMMARY ---\n",
            "Successful: 2/5\n",
            "Valid Syntax Rate: 0.400\n",
            "Failed IDs: ['no_cadquery_object', 'runtime_error', 'syntax_error']\n",
            "\n",
            "Overall VSR: 40.0%\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import cadquery as cq\n",
        "import numpy as np\n",
        "import textwrap\n",
        "from typing import Union, Dict, List\n",
        "\n",
        "os.environ[\"CADQUERY_LOG_LEVEL\"] = \"ERROR\"\n",
        "\n",
        "\n",
        "def _load_solid_from_code(\n",
        "    code: str, script_id: str = \"unknown\"\n",
        ") -> Union[cq.Solid, cq.Compound]:\n",
        "    \"\"\"Execute Python code and return any CadQuery object found.\"\"\"\n",
        "    # Clean up indentation issues\n",
        "    cleaned_code = textwrap.dedent(code).strip()\n",
        "\n",
        "    # Provide necessary imports in the execution namespace\n",
        "    ns = {\"cq\": cq, \"cadquery\": cq, \"np\": np, \"numpy\": np, \"__builtins__\": __builtins__}\n",
        "    try:\n",
        "        exec(cleaned_code, ns)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error executing script {script_id}: {e}\")\n",
        "\n",
        "    # Find any CadQuery objects in the namespace\n",
        "    cadquery_objects = []\n",
        "    for var_name, var_value in ns.items():\n",
        "        if isinstance(var_value, (cq.Workplane, cq.Solid, cq.Compound)):\n",
        "            cadquery_objects.append((var_name, var_value))\n",
        "\n",
        "    if not cadquery_objects:\n",
        "        raise ValueError(\n",
        "            f\"No CadQuery objects (Workplane, Solid, or Compound) found in script {script_id}\"\n",
        "        )\n",
        "\n",
        "    if len(cadquery_objects) > 1:\n",
        "        # If multiple objects, prefer common names\n",
        "        preferred_names = [\"solid\", \"result\", \"shape\", \"part\", \"object\", \"obj\", \"res\"]\n",
        "        for preferred in preferred_names:\n",
        "            for var_name, var_value in cadquery_objects:\n",
        "                if var_name == preferred:\n",
        "                    cadquery_objects = [(var_name, var_value)]\n",
        "                    break\n",
        "            if len(cadquery_objects) == 1:\n",
        "                break\n",
        "\n",
        "        # If still multiple, just take the first one but warn\n",
        "        if len(cadquery_objects) > 1:\n",
        "            var_names = [name for name, _ in cadquery_objects]\n",
        "            print(\n",
        "                f\"Warning: Multiple CadQuery objects found in {script_id}: {var_names}. Using '{cadquery_objects[0][0]}'\"\n",
        "            )\n",
        "\n",
        "    var_name, solid_obj = cadquery_objects[0]\n",
        "\n",
        "    # Handle different CadQuery object types\n",
        "    if isinstance(solid_obj, cq.Workplane):\n",
        "        # Extract the solid from the workplane\n",
        "        solid_obj = solid_obj.val()\n",
        "\n",
        "    # Handle Compound objects (multiple solids combined)\n",
        "    if hasattr(solid_obj, \"Solids\") and callable(getattr(solid_obj, \"Solids\")):\n",
        "        solids = solid_obj.Solids()\n",
        "        if len(solids) == 1:\n",
        "            solid_obj = solids[0]\n",
        "        elif len(solids) > 1:\n",
        "            # If multiple solids, we need to combine them into one\n",
        "            # Use the compound itself if it's valid for our purposes\n",
        "            pass  # Keep the compound as is\n",
        "        else:\n",
        "            raise ValueError(f\"No solids found in compound in script {script_id}\")\n",
        "\n",
        "    # Accept both Solid and Compound objects for our mesh operations\n",
        "    if not isinstance(solid_obj, (cq.Solid, cq.Compound)):\n",
        "        raise ValueError(\n",
        "            f\"CadQuery object '{var_name}' is not a Solid or Compound object in script {script_id}, got {type(solid_obj)}\"\n",
        "        )\n",
        "\n",
        "    return solid_obj\n",
        "\n",
        "\n",
        "def evaluate_syntax_rate(\n",
        "    codes: Dict[str, str], verbose: bool = True\n",
        ") -> Dict[str, Union[float, int, List[str]]]:\n",
        "    \"\"\"Evaluate valid syntax rate for a dictionary of CadQuery code strings.\n",
        "\n",
        "    Args:\n",
        "        codes: Dict with IDs as keys and Python code strings as values\n",
        "        verbose: Whether to print detailed results\n",
        "\n",
        "    Returns:\n",
        "        Dict with 'vsr' (valid syntax rate), 'successful' (count), 'total' (count),\n",
        "        'failed_ids' (list of IDs that failed)\n",
        "    \"\"\"\n",
        "    if not codes:\n",
        "        if verbose:\n",
        "            print(\"No code provided\")\n",
        "        return {\"vsr\": 0.0, \"successful\": 0, \"total\": 0, \"failed_ids\": []}\n",
        "\n",
        "    ids = sorted(codes.keys())\n",
        "    successful_count = 0\n",
        "    failed_ids = []\n",
        "\n",
        "    for script_id in ids:\n",
        "        code = codes[script_id]\n",
        "        try:\n",
        "            solid = _load_solid_from_code(code, script_id)\n",
        "            successful_count += 1\n",
        "            if verbose:\n",
        "                print(f\"✓ {script_id}: Successfully executed\")\n",
        "        except Exception as exc:\n",
        "            failed_ids.append(script_id)\n",
        "            if verbose:\n",
        "                print(f\"✗ {script_id}: {exc}\")\n",
        "\n",
        "    total_count = len(ids)\n",
        "    vsr = successful_count / total_count if total_count > 0 else 0.0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n--- SUMMARY ---\")\n",
        "        print(f\"Successful: {successful_count}/{total_count}\")\n",
        "        print(f\"Valid Syntax Rate: {vsr:.3f}\")\n",
        "        if failed_ids:\n",
        "            print(f\"Failed IDs: {failed_ids}\")\n",
        "\n",
        "    return {\n",
        "        \"vsr\": vsr,\n",
        "        \"successful\": successful_count,\n",
        "        \"total\": total_count,\n",
        "        \"failed_ids\": failed_ids,\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_syntax_rate_simple(codes: Dict[str, str]) -> float:\n",
        "    \"\"\"Simple function that just returns the valid syntax rate as a float.\"\"\"\n",
        "    result = evaluate_syntax_rate(codes, verbose=False)\n",
        "    return result[\"vsr\"]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test cases\n",
        "    test_codes = {\n",
        "        \"simple_box\": \"\"\"\n",
        "            height = 60.0\n",
        "            width = 80.0\n",
        "            thickness = 10.0\n",
        "            result = cq.Workplane(\"XY\").box(height, width, thickness)\n",
        "        \"\"\",\n",
        "        \"box_with_hole\": \"\"\"\n",
        "            height = 60.0\n",
        "            width = 80.0\n",
        "            thickness = 10.0\n",
        "            diameter = 22.0\n",
        "            padding = 12.0\n",
        "\n",
        "            # make the base\n",
        "            result = (\n",
        "                cq.Workplane(\"XY\")\n",
        "                .box(height, width, thickness)\n",
        "                .faces(\">Z\")\n",
        "                .workplane()\n",
        "                .hole(diameter)\n",
        "                .faces(\">Z\")\n",
        "                .workplane()\n",
        "                .rect(height - padding, width - padding, forConstruction=True)\n",
        "                .vertices()\n",
        "                .cboreHole(2.4, 4.4, 2.1)\n",
        "            )\n",
        "        \"\"\",\n",
        "        \"syntax_error\": \"\"\"\n",
        "            result = cq.Workplane(\"XY\").box(10, 10, 10\n",
        "            # Missing closing parenthesis\n",
        "        \"\"\",\n",
        "        \"runtime_error\": \"\"\"\n",
        "            result = cq.Workplane(\"XY\").box(undefined_variable, 10, 10)\n",
        "        \"\"\",\n",
        "        \"no_cadquery_object\": \"\"\"\n",
        "            x = 5\n",
        "            y = 10\n",
        "            z = x + y\n",
        "        \"\"\",\n",
        "    }\n",
        "\n",
        "    print(\"Testing Valid Syntax Rate evaluation:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    result = evaluate_syntax_rate(test_codes)\n",
        "    print(f\"\\nOverall VSR: {result['vsr']:.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "66229460252d40279aa4f776f991eade",
            "9896c4c321c341f38aec23ab1c557e14",
            "56d565b2c1784434903d110780bd3689",
            "1447c5abd22b4237bd8635735f358cfa",
            "6c990d2a84cd4fca9a6750b7ff4cf2cd",
            "869270b5677c4fb9bbbfb7613cc7d625",
            "e1135f728b244ae6a473fa10856c8e17",
            "176be921d8674d03b351b85bcb9bf982",
            "73f6a2b5dc21451fb28a4813f86c492a",
            "937576868de94847b2efa54e88a897ea",
            "d0bcea21ddd9444899137f116bd7261b"
          ]
        },
        "id": "WQYKRllmH8P8",
        "outputId": "08c502cb-d8b7-4a6d-df20-3fa0164050a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading dataset splits …\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-14-453126151.py:293: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66229460252d40279aa4f776f991eade",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train 1:   0%|          | 0/73645 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-14-453126151.py:255: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, sys, random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "from datasets import load_dataset, disable_caching\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import (\n",
        "    GPT2Config, GPT2LMHeadModel, PreTrainedTokenizerFast,\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "from huggingface_hub import login\n",
        "\n",
        "if sys.version_info < (3, 9):\n",
        "    raise RuntimeError(\"Python ≥3.9 required – please upgrade your runtime.\")\n",
        "\n",
        "login(token=os.getenv(\"HF_TOKEN\", \"\"))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "use_amp = torch.cuda.is_available()\n",
        "\n",
        "disable_caching()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # data\n",
        "    dataset_name: str = \"CADCODER/GenCAD-Code\"\n",
        "    cache_dir: str = \"./hf_cache\"\n",
        "    tokenizer_slice: str = \"train\"\n",
        "\n",
        "    # model IO sizes\n",
        "    img_size: int = 224\n",
        "    max_len: int = 512\n",
        "\n",
        "    # train hyper‑params\n",
        "    batch_size: int = 2\n",
        "    num_workers: int = 2\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-2\n",
        "    epochs: int = 1\n",
        "\n",
        "    decode_max_len: int = 256\n",
        "    beam_size: int = 1\n",
        "\n",
        "    freeze_vision: bool = True\n",
        "\n",
        "cfg = Config()\n",
        "Path(cfg.cache_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "TOKENIZER_JSON = Path(\"tokenizer/tokenizer.json\")\n",
        "TOKENIZER_JSON.parent.mkdir(exist_ok=True)\n",
        "\n",
        "if not TOKENIZER_JSON.exists():\n",
        "    print(\"Tokenizer not found – training Byte‑level BPE (first run)…\")\n",
        "    ds_sample = load_dataset(cfg.dataset_name, split=cfg.tokenizer_slice,\n",
        "                             cache_dir=cfg.cache_dir)\n",
        "    codes_iter = (row[\"cadquery\"] for row in ds_sample.select(range(min(100_000, len(ds_sample)))))\n",
        "    tok = ByteLevelBPETokenizer()\n",
        "    tok.train_from_iterator(codes_iter, vocab_size=32_768, min_frequency=2,\n",
        "                            special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n",
        "    tok.save(str(TOKENIZER_JSON))\n",
        "    print(\"✓ Tokenizer saved →\", TOKENIZER_JSON)\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_JSON),\n",
        "                                       bos_token=\"<s>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
        "\n",
        "\n",
        "class CadQueryDataset(Dataset):\n",
        "    \"\"\"Minimal dataset wrapper that returns image tensor + GPT token IDs.\"\"\"\n",
        "\n",
        "    def __init__(self, split: str):\n",
        "        self.ds = load_dataset(cfg.dataset_name, split=split, cache_dir=cfg.cache_dir)\n",
        "        is_train = split == \"train\"\n",
        "        self.tf = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(cfg.img_size, scale=(0.8, 1.0), antialias=True)\n",
        "            if is_train else transforms.Resize((cfg.img_size, cfg.img_size), antialias=True),\n",
        "            transforms.RandomHorizontalFlip() if is_train else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        item = self.ds[idx]\n",
        "        img = self.tf(item[\"image\"])\n",
        "        ids = hf_tokenizer.encode(item[\"cadquery\"], truncation=True,\n",
        "                                  max_length=cfg.max_len, add_special_tokens=False)\n",
        "        return {\"pixel_values\": img, \"labels\": torch.tensor(ids, dtype=torch.long)}\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "    imgs = torch.stack([b[\"pixel_values\"] for b in batch])\n",
        "    lens = [len(b[\"labels\"]) for b in batch]\n",
        "    max_len = max(lens)\n",
        "    labels = torch.full((len(batch), max_len), hf_tokenizer.pad_token_id, dtype=torch.long)\n",
        "    for i, b in enumerate(batch):\n",
        "        labels[i, :lens[i]] = b[\"labels\"]\n",
        "    return {\"pixel_values\": imgs, \"labels\": labels}\n",
        "\n",
        "print(\"Loading dataset splits …\")\n",
        "train_loader = DataLoader(CadQueryDataset(\"train\"),\n",
        "                          batch_size=cfg.batch_size, shuffle=True,\n",
        "                          num_workers=cfg.num_workers, pin_memory=True,\n",
        "                          collate_fn=collate_fn)\n",
        "val_loader = DataLoader(CadQueryDataset(\"validation\"),\n",
        "                        batch_size=cfg.batch_size, shuffle=False,\n",
        "                        num_workers=cfg.num_workers, pin_memory=True,\n",
        "                        collate_fn=collate_fn)\n",
        "\n",
        "class ResNetEncoder(nn.Module):\n",
        "    def __init__(self, freeze: bool = True):\n",
        "        super().__init__()\n",
        "        rn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "        self.features = nn.Sequential(*list(rn.children())[:-1])\n",
        "        self.proj = nn.Linear(rn.fc.in_features, 1024)\n",
        "        if freeze:\n",
        "            for p in self.features.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.proj(torch.flatten(self.features(x), 1))\n",
        "\n",
        "\n",
        "\n",
        "class Vision2Code(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNetEncoder(cfg.freeze_vision)\n",
        "        gpt_cfg = GPT2Config(\n",
        "            vocab_size=len(hf_tokenizer),\n",
        "            n_positions=cfg.max_len,\n",
        "            n_ctx=cfg.max_len,\n",
        "            n_layer=8,\n",
        "            n_head=8,\n",
        "            n_embd=autoregressive_dim,\n",
        "            bos_token_id=hf_tokenizer.bos_token_id,\n",
        "            eos_token_id=hf_tokenizer.eos_token_id,\n",
        "            pad_token_id=hf_tokenizer.pad_token_id,\n",
        "        )\n",
        "        self.decoder = GPT2LMHeadModel(gpt_cfg)\n",
        "        self.vis_proj = nn.Linear(autoregressive_dim, autoregressive_dim)\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor, labels: torch.Tensor | None = None):\n",
        "        B = pixel_values.size(0)\n",
        "        prefix = self.vis_proj(self.encoder(pixel_values)).unsqueeze(1)\n",
        "\n",
        "        if labels is not None:\n",
        "            bos = torch.full((B, 1), hf_tokenizer.bos_token_id, device=pixel_values.device)\n",
        "            inp_tok = torch.cat([bos, labels], dim=1)[:, :cfg.max_len]\n",
        "            tok_emb = self.decoder.transformer.wte(inp_tok)\n",
        "            inputs_embeds = torch.cat([prefix, tok_emb], dim=1)[:, :cfg.max_len]\n",
        "\n",
        "            ignore = torch.full((B, inputs_embeds.size(1) - labels.size(1)), -100,\n",
        "                                dtype=torch.long, device=pixel_values.device)\n",
        "            dec_labels = torch.cat([ignore, labels], dim=1)[:, :cfg.max_len]\n",
        "            dec_labels[dec_labels == hf_tokenizer.pad_token_id] = -100\n",
        "            return self.decoder(inputs_embeds=inputs_embeds, labels=dec_labels)\n",
        "\n",
        "        bos_emb = self.decoder.transformer.wte(torch.full((B, 1), hf_tokenizer.bos_token_id,\n",
        "                                                          device=pixel_values.device))\n",
        "        inputs_embeds = torch.cat([prefix, bos_emb], dim=1)\n",
        "        gen = self.decoder.generate(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            max_length=cfg.decode_max_len,\n",
        "            num_beams=cfg.beam_size,\n",
        "            pad_token_id=hf_tokenizer.pad_token_id,\n",
        "            eos_token_id=hf_tokenizer.eos_token_id,\n",
        "        )\n",
        "        return gen[:, 1:]\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def ids_to_str(ids: torch.Tensor) -> str:\n",
        "    \"\"\"Convert a 1‑D tensor of IDs to string, stripping special tokens.\"\"\"\n",
        "    return hf_tokenizer.decode(ids.tolist(), clean_up_tokenization_spaces=True,\n",
        "                               skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_syntax_rate_simple(preds: Dict[str, str]) -> float:\n",
        "    \"\"\"% of generated strings that are non‑empty and compile‑looking.\"\"\"\n",
        "    good = sum(1 for v in preds.values() if v.strip())\n",
        "    return good / max(len(preds), 1)\n",
        "\n",
        "def get_iou_best(ref: str, pred: str) -> float:\n",
        "    \"\"\"Dummy IoU placeholder (returns 0 unless strings identical).\"\"\"\n",
        "    return float(ref.strip() == pred.strip())\n",
        "\n",
        "\n",
        "def train_epoch(model: nn.Module, loader, opt, scaler, epoch: int):\n",
        "    model.train()\n",
        "    pbar = tqdm(loader, desc=f\"train {epoch}\")\n",
        "    for batch in pbar:\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            out = model(batch[\"pixel_values\"].to(device, non_blocking=True),\n",
        "                         batch[\"labels\"].to(device, non_blocking=True))\n",
        "            loss = out.loss\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, loader, max_batches: int = 200):\n",
        "    model.eval()\n",
        "    preds, refs = {}, {}\n",
        "    with torch.no_grad():\n",
        "        for b_idx, batch in enumerate(loader):\n",
        "            if b_idx >= max_batches:\n",
        "                break\n",
        "            pix = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
        "            gen_ids = model(pix)\n",
        "            for i, (g, r) in enumerate(zip(gen_ids, batch[\"labels\"])):\n",
        "                key = f\"{b_idx:04d}_{i:02d}\"\n",
        "                preds[key] = ids_to_str(g)\n",
        "                refs[key] = ids_to_str(r[r != hf_tokenizer.pad_token_id])\n",
        "\n",
        "    vsr = evaluate_syntax_rate_simple(preds) * 100.0\n",
        "    ious = [get_iou_best(refs[k], preds[k]) for k in preds]\n",
        "    mean_iou = sum(ious) / len(ious) if ious else 0.0\n",
        "    return vsr, mean_iou\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    set_seed()\n",
        "    model = Vision2Code().to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate,\n",
        "                            weight_decay=cfg.weight_decay)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    for ep in range(1, cfg.epochs + 1):\n",
        "        train_epoch(model, train_loader, opt, scaler, ep)\n",
        "        vsr, iou = evaluate(model, val_loader)\n",
        "        print(f\"✦ Epoch {ep}  |  VSR {vsr:5.1f}%   IoU {iou:.3f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"vision2cadquery.pt\")\n",
        "    print(\"Training complete – model saved to vision2cadquery.pt\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljyja9KXZHHJ"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nI09QwYZIiu"
      },
      "outputs": [],
      "source": [
        "import argparse, torch\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from vision2cadquery_updated import Vision2Code, TOKENIZER_JSON, hf_tokenizer, cfg\n",
        "\n",
        "\n",
        "def ids_to_str(ids):\n",
        "    return hf_tokenizer.decode(ids.tolist(), skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def get_test_loader(batch_size=4):\n",
        "    tf = transforms.Compose([\n",
        "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
        "    ])\n",
        "\n",
        "    def collate(batch):\n",
        "        images = torch.stack([tf(b[\"image\"]) for b in batch])\n",
        "        labels = [hf_tokenizer.encode(b[\"cadquery\"], truncation=True, max_length=cfg.max_len)\n",
        "                  for b in batch]\n",
        "        return {\"pixel_values\": images, \"labels\": labels}\n",
        "\n",
        "    ds = load_dataset(cfg.dataset_name, split=\"test\", cache_dir=cfg.cache_dir)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(weights_path: str):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = Vision2Code().to(device)\n",
        "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    loader = get_test_loader()\n",
        "\n",
        "    preds, refs = {}, {}\n",
        "    for b_idx, batch in enumerate(loader):\n",
        "        pix = batch[\"pixel_values\"].to(device)\n",
        "        gen_ids = model(pix)\n",
        "        for i, (g, r) in enumerate(zip(gen_ids, batch[\"labels\"])):\n",
        "            key = f\"{b_idx:04d}_{i:02d}\"\n",
        "            preds[key] = ids_to_str(g)\n",
        "            refs[key] = ids_to_str(torch.tensor(r))\n",
        "\n",
        "    vsr = evaluate_syntax_rate_simple(preds) * 100.0\n",
        "    mean_iou = sum(get_iou_best(refs[k], preds[k]) for k in preds) / len(preds)\n",
        "    print(f\"Test split  VSR {vsr:5.1f}%   IoU {mean_iou:.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = argparse.ArgumentParser(description=\"Evaluate Vision2CadQuery on the test split.\")\n",
        "    p.add_argument(\"weights\", nargs=\"?\", default=\"vision2cadquery.pt\",\n",
        "                   help=\"Path to the .pt checkpoint (default: vision2cadquery.pt)\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    evaluate(args.weights)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1447c5abd22b4237bd8635735f358cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_937576868de94847b2efa54e88a897ea",
            "placeholder": "​",
            "style": "IPY_MODEL_d0bcea21ddd9444899137f116bd7261b",
            "value": " 2014/73645 [04:58&lt;2:56:29,  6.76it/s, loss=2.5392]"
          }
        },
        "176be921d8674d03b351b85bcb9bf982": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d565b2c1784434903d110780bd3689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_176be921d8674d03b351b85bcb9bf982",
            "max": 73645,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73f6a2b5dc21451fb28a4813f86c492a",
            "value": 2014
          }
        },
        "66229460252d40279aa4f776f991eade": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9896c4c321c341f38aec23ab1c557e14",
              "IPY_MODEL_56d565b2c1784434903d110780bd3689",
              "IPY_MODEL_1447c5abd22b4237bd8635735f358cfa"
            ],
            "layout": "IPY_MODEL_6c990d2a84cd4fca9a6750b7ff4cf2cd"
          }
        },
        "6c990d2a84cd4fca9a6750b7ff4cf2cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f6a2b5dc21451fb28a4813f86c492a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "869270b5677c4fb9bbbfb7613cc7d625": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937576868de94847b2efa54e88a897ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9896c4c321c341f38aec23ab1c557e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869270b5677c4fb9bbbfb7613cc7d625",
            "placeholder": "​",
            "style": "IPY_MODEL_e1135f728b244ae6a473fa10856c8e17",
            "value": "train 1:   3%"
          }
        },
        "d0bcea21ddd9444899137f116bd7261b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1135f728b244ae6a473fa10856c8e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
